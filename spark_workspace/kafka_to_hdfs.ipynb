{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "kafka = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1\"\n",
    "spark = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\"--packages {},{} pyspark-shell\".format(kafka, spark))\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e3aab90b-33fd-4c7b-b53b-6fc6114b3b52;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      ":: resolution report :: resolve 443ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.16 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 by [org.apache.kafka#kafka-clients;2.4.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   2   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e3aab90b-33fd-4c7b-b53b-6fc6114b3b52\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/12ms)\n",
      "21/12/28 11:58:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/28 11:58:50 WARN TransportClientFactory: DNS resolution failed for master:7077 took 10012 ms\n",
      "21/12/28 11:58:50 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to master:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: java.net.UnknownHostException: master\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)\n",
      "\tat java.base/java.net.InetAddress.getByName(InetAddress.java:1252)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n",
      "\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n",
      "\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n",
      "\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n",
      "\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:605)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n",
      "\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:1008)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:516)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "21/12/28 11:59:00 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to master:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: java.net.UnknownHostException: master\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)\n",
      "\tat java.base/java.net.InetAddress.getByName(InetAddress.java:1252)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n",
      "\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n",
      "\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n",
      "\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n",
      "\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:605)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n",
      "\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:1008)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:516)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/28 11:59:30 WARN TransportClientFactory: DNS resolution failed for master:7077 took 10008 ms\n",
      "21/12/28 11:59:30 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master master:7077\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:107)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to master:7077\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:204)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:202)\n",
      "\tat org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:198)\n",
      "\t... 4 more\n",
      "Caused by: java.net.UnknownHostException: master\n",
      "\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)\n",
      "\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)\n",
      "\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)\n",
      "\tat java.base/java.net.InetAddress.getByName(InetAddress.java:1252)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)\n",
      "\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)\n",
      "\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)\n",
      "\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)\n",
      "\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)\n",
      "\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)\n",
      "\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)\n",
      "\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)\n",
      "\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:605)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n",
      "\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:1008)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:516)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "21/12/28 11:59:40 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "21/12/28 11:59:40 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "21/12/28 11:59:40 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n",
      "21/12/28 11:59:40 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32/2589019438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mKAFKA_BROKER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"kafka-container:9092\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mKAFKA_TOPIC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"item-shopee\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shopee_analysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark://master:7077\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[1;32m    147\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1574\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:89)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "KAFKA_BROKER = \"kafka-container:9092\"\n",
    "KAFKA_TOPIC = \"item-shopee\"\n",
    "spark = SparkSession.builder.appName(\"shopee_analysis\").master(\"spark://master:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaMessages = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = kafkaMessages.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/28 11:55:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b7e935b8-18dd-4048-bad7-909635d1bad5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/12/28 11:55:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "21/12/28 11:55:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:55:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:55:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:55:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:55:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:55:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:55:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:55:58 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:55:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:55:59 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:00 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:01 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:02 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:04 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:05 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:07 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/28 11:56:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:08 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:09 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:10 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:11 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:12 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:13 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:14 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n",
      "21/12/28 11:56:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "21/12/28 11:56:16 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0-1, groupId=spark-kafka-source-ed5a8a7e-8103-4fa8-8f95-ab98d15a0adf--2037279542-driver-0] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected\n"
     ]
    }
   ],
   "source": [
    "rawQuery = message \\\n",
    "        .writeStream \\\n",
    "        .queryName(\"qraw2\")\\\n",
    "        .format(\"memory\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0136fb98878f2c037a786f1e346cbebb20f85b5d56d3fdaaf56b1008ea37aa39"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
